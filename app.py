import streamlit as st
import torch
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image
import requests

# C·∫•u h√¨nh Streamlit
st.set_page_config(page_title="Image Captioner", layout="centered", page_icon="üñºÔ∏è")

# T·∫£i m√¥ h√¨nh, feature extractor v√† tokenizer
@st.cache_resource
def load_model():
    model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    return model, feature_extractor, tokenizer

# H√†m d·ª± ƒëo√°n caption t·ª´ ·∫£nh
def predict_caption(image, model, feature_extractor, tokenizer):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
    pixel_values = feature_extractor(images=[image], return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)

    # Sinh caption
    output_ids = model.generate(pixel_values, max_length=16, num_beams=4)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption

# ·ª®ng d·ª•ng ch√≠nh
def main():
    # Load m√¥ h√¨nh v√† c√°c c√¥ng c·ª•
    model, feature_extractor, tokenizer = load_model()

    # Giao di·ªán Streamlit
    st.title("üñºÔ∏è Image Captioner")
    st.write("### T·∫°o ch√∫ th√≠ch cho ·∫£nh c·ªßa b·∫°n b·∫±ng m√¥ h√¨nh AI hi·ªán ƒë·∫°i!")

    # Input: D√°n URL ho·∫∑c upload ·∫£nh
    st.markdown("### Ch·ªçn m·ªôt ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu")
    image_url = st.text_input("D√°n URL ·∫£nh:")
    uploaded_file = st.file_uploader("Ho·∫∑c t·∫£i l√™n m·ªôt ·∫£nh t·ª´ m√°y t√≠nh", type=["jpg", "png", "jpeg"])

    # X·ª≠ l√Ω ·∫£nh
    if uploaded_file or image_url:
        try:
            # Load ·∫£nh
            if uploaded_file:
                image = Image.open(uploaded_file).convert("RGB")
            else:
                response = requests.get(image_url, stream=True)
                image = Image.open(response.raw).convert("RGB")

            # Hi·ªÉn th·ªã ·∫£nh
            st.image(image, caption="·∫¢nh c·ªßa b·∫°n", use_column_width=True)

            # D·ª± ƒëo√°n caption
            with st.spinner("üîÑ ƒêang t·∫°o caption..."):
                caption = predict_caption(image, model, feature_extractor, tokenizer)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.success("üéâ Caption ƒë√£ ƒë∆∞·ª£c t·∫°o:")
            st.write(f"**{caption}**")

        except Exception as e:
            st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω ·∫£nh: {e}")

    # Footer
    st.markdown("---")
    st.markdown("Made with ‚ù§Ô∏è by [GROUP 5 FROM 24TNT-HCMUS]")

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    main()
